{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uttam112/sprinklr/blob/main/Creating_Chunks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installing Transformers and Langchain"
      ],
      "metadata": {
        "id": "KXbwnhLuClIO"
      },
      "id": "KXbwnhLuClIO"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install langchain"
      ],
      "metadata": {
        "id": "-Qiag1mfl6I5"
      },
      "execution_count": null,
      "outputs": [],
      "id": "-Qiag1mfl6I5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Function for splitting text into chunks"
      ],
      "metadata": {
        "id": "oUhWNdHXDBYY"
      },
      "id": "oUhWNdHXDBYY"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import GPT2TokenizerFast"
      ],
      "metadata": {
        "id": "zpJW5uZZn_5g"
      },
      "execution_count": null,
      "outputs": [],
      "id": "zpJW5uZZn_5g"
    },
    {
      "cell_type": "code",
      "source": [
        "#Create function to count tokens\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    return len(tokenizer.encode(text))\n",
        "\n",
        "\n",
        "# Split text into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "\n",
        "    chunk_size = 800,                #chunk size of each document\n",
        "    chunk_overlap  = 100,            #An overlap from previous document to have some context\n",
        "    length_function = count_tokens,  #there is also a default length_function -> can remove this line\n",
        ")"
      ],
      "metadata": {
        "id": "wa_5_pu4oSKB"
      },
      "execution_count": null,
      "outputs": [],
      "id": "wa_5_pu4oSKB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the documents and creating chunks"
      ],
      "metadata": {
        "id": "dL5EHfnPDVmG"
      },
      "id": "dL5EHfnPDVmG"
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = []\n",
        "for i in range(16):\n",
        "  filename = \"new_content/context\"+str(i)+\".txt\"         #change according to your file path\n",
        "  text_file = open(filename, \"r\")\n",
        "  #read whole file to a string\n",
        "  print(i)\n",
        "  data = text_file.read()\n",
        "  chunks = chunks + text_splitter.create_documents([data])\n",
        "\n",
        "#================================================for debugging purpose===========================================================\n",
        "# text_file = open(\"context1.txt\", \"r\")\n",
        "# #read whole file to a string\n",
        "# data = text_file.read()\n",
        "# chunks = text_splitter.create_documents([data])\n",
        "\n",
        "# text_file = open(\"context2.txt\", \"r\")\n",
        "# #read whole file to a string\n",
        "# data = text_file.read()\n",
        "# chunks2 = text_splitter.create_documents([data])\n",
        "\n",
        "# text_file = open(\"context3.txt\", \"r\")\n",
        "# #read whole file to a string\n",
        "# data = text_file.read()\n",
        "# chunks3 = text_splitter.create_documents([data])\n",
        "\n",
        "# text_file = open(\"context4.txt\", \"r\")\n",
        "# #read whole file to a string\n",
        "# data = text_file.read()\n",
        "# chunks4 = text_splitter.create_documents([data])\n",
        "\n",
        "# text_file = open(\"context5.txt\", \"r\")\n",
        "# #read whole file to a string\n",
        "# data = text_file.read()\n",
        "# chunks5 = text_splitter.create_documents([data])\n",
        "\n",
        "# text_file = open(\"context6.txt\", \"r\")\n",
        "# #read whole file to a string\n",
        "# data = text_file.read()\n",
        "# chunks6 = text_splitter.create_documents([data])\n",
        "\n",
        "# text_file = open(\"context7.txt\", \"r\")\n",
        "# #read whole file to a string\n",
        "# data = text_file.read()\n",
        "# chunks7 = text_splitter.create_documents([data])\n",
        "\n",
        "# text_file = open(\"context8.txt\", \"r\")\n",
        "# #read whole file to a string\n",
        "# data = text_file.read()\n",
        "# chunks8 = text_splitter.create_documents([data])\n",
        "\n",
        "\n",
        "# text_file = open(\"context9.txt\", \"r\")\n",
        "# #read whole file to a string\n",
        "# data = text_file.read()\n",
        "# chunks9 = text_splitter.create_documents([data])\n",
        "\n",
        "# text_file = open(\"context10.txt\", \"r\")\n",
        "# #read whole file to a string\n",
        "# data = text_file.read()\n",
        "# chunks10 = text_splitter.create_documents([data])\n",
        "\n",
        "# text_file = open(\"context11.txt\", \"r\")\n",
        "# #read whole file to a string\n",
        "# data = text_file.read()\n",
        "# chunks11 = text_splitter.create_documents([data])\n",
        "\n",
        "# chunks = chunks  + chunks2 + chunks3 + chunks4 + chunks5 + chunks6 + chunks7 + chunks8 + chunks9 + chunks10 + chunks11"
      ],
      "metadata": {
        "id": "6WIGYhL7n_-e"
      },
      "execution_count": null,
      "outputs": [],
      "id": "6WIGYhL7n_-e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Huggingface embeddings/Sentence Transformer Embeddings"
      ],
      "metadata": {
        "id": "Fvlh0qaAD4zA"
      },
      "id": "Fvlh0qaAD4zA"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings()"
      ],
      "metadata": {
        "id": "ybu0vqQ5DpSB"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ybu0vqQ5DpSB"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "db = FAISS.from_documents(chunks, embeddings)"
      ],
      "metadata": {
        "id": "krPVl3URpzO-"
      },
      "execution_count": null,
      "outputs": [],
      "id": "krPVl3URpzO-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization of Token count in each Chunk"
      ],
      "metadata": {
        "id": "d2AOdW65KPjo"
      },
      "id": "d2AOdW65KPjo"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a list of token counts\n",
        "token_counts = [count_tokens(chunk.page_content) for chunk in chunks]\n",
        "\n",
        "# Create a DataFrame from the token counts\n",
        "df = pd.DataFrame({'Token Count': token_counts})\n",
        "\n",
        "# Create a histogram of the token count distribution\n",
        "df.hist(bins=60, )\n",
        "\n",
        "plt.xlabel(\"Token Count\")\n",
        "plt.ylabel(\"Number of Chunks\")\n",
        "\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mkoi4uTr7uBT"
      },
      "execution_count": null,
      "outputs": [],
      "id": "mkoi4uTr7uBT"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}